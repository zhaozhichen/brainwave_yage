from http.server import BaseHTTPRequestHandler
import json
import os
import uuid
import time
from openai import OpenAI

# In-memory storage for audio sessions (in production, use Redis or database)
audio_sessions = {}

class handler(BaseHTTPRequestHandler):
    def do_GET(self):
        try:
            # Serve the UI for root path
            if self.path == "/":
                self.send_response(200)
                self.send_header('Content-type', 'text/html')
                self.end_headers()
                
                # Embedded HTML content
                html_content = """<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Brainwave API - AI Text Processing</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 40px rgba(0,0,0,0.1);
            overflow: hidden;
        }

        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }

        .header h1 {
            font-size: 2.5rem;
            margin-bottom: 10px;
        }

        .header p {
            font-size: 1.1rem;
            opacity: 0.9;
        }

        .content {
            padding: 40px;
        }

        .tabs {
            display: flex;
            margin-bottom: 30px;
            border-bottom: 2px solid #f0f0f0;
        }

        .tab {
            padding: 15px 30px;
            cursor: pointer;
            border: none;
            background: none;
            font-size: 1rem;
            font-weight: 500;
            color: #666;
            transition: all 0.3s ease;
        }

        .tab.active {
            color: #667eea;
            border-bottom: 3px solid #667eea;
        }

        .tab:hover {
            color: #667eea;
        }

        .tab-content {
            display: none;
        }

        .tab-content.active {
            display: block;
        }

        .form-group {
            margin-bottom: 25px;
        }

        label {
            display: block;
            margin-bottom: 8px;
            font-weight: 600;
            color: #333;
        }

        textarea {
            width: 100%;
            min-height: 150px;
            padding: 15px;
            border: 2px solid #e0e0e0;
            border-radius: 10px;
            font-size: 1rem;
            font-family: inherit;
            resize: vertical;
            transition: border-color 0.3s ease;
        }

        textarea:focus {
            outline: none;
            border-color: #667eea;
        }

        .btn {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            padding: 15px 30px;
            border-radius: 10px;
            font-size: 1rem;
            font-weight: 600;
            cursor: pointer;
            transition: transform 0.2s ease;
            margin-right: 10px;
        }

        .btn:hover {
            transform: translateY(-2px);
        }

        .btn:disabled {
            opacity: 0.6;
            cursor: not-allowed;
            transform: none;
        }

        .btn.recording {
            background: linear-gradient(135deg, #e74c3c 0%, #c0392b 100%);
        }

        .btn.stop {
            background: linear-gradient(135deg, #e74c3c 0%, #c0392b 100%);
        }

        .result {
            margin-top: 30px;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 10px;
            border-left: 4px solid #667eea;
        }

        .result h3 {
            margin-bottom: 15px;
            color: #333;
        }

        .result-content {
            white-space: pre-wrap;
            line-height: 1.6;
            color: #555;
        }

        .loading {
            text-align: center;
            padding: 20px;
            color: #666;
        }

        .error {
            background: #fee;
            border-left-color: #e74c3c;
            color: #c0392b;
        }

        .status {
            text-align: center;
            padding: 20px;
            background: #e8f5e8;
            border-radius: 10px;
            margin-bottom: 20px;
            color: #2d5a2d;
        }

        .api-status {
            display: inline-block;
            width: 10px;
            height: 10px;
            border-radius: 50%;
            margin-right: 10px;
        }

        .api-status.online {
            background: #27ae60;
        }

        .api-status.offline {
            background: #e74c3c;
        }

        .audio-controls {
            display: flex;
            gap: 10px;
            margin-bottom: 20px;
        }

        .recording-status {
            padding: 10px;
            border-radius: 5px;
            margin-bottom: 20px;
            text-align: center;
            font-weight: 600;
        }

        .recording-status.recording {
            background: #fee;
            color: #c0392b;
        }

        .recording-status.processing {
            background: #fff3cd;
            color: #856404;
        }

        .recording-status.complete {
            background: #d4edda;
            color: #155724;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>üß† Brainwave API</h1>
            <p>AI-Powered Text Processing & Analysis</p>
        </div>

        <div class="content">
            <div class="status" id="status">
                <span class="api-status" id="apiStatus"></span>
                <span id="statusText">Checking API status...</span>
            </div>

            <div class="tabs">
                <button class="tab active" onclick="showTab('readability')">üìù Readability</button>
                <button class="tab" onclick="showTab('ask-ai')">ü§ñ Ask AI</button>
                <button class="tab" onclick="showTab('correctness')">‚úÖ Fact Check</button>
                <button class="tab" onclick="showTab('realtime')">üé§ Real-time Audio</button>
            </div>

            <!-- Readability Tab -->
            <div id="readability" class="tab-content active">
                <h2>Enhance Text Readability</h2>
                <p style="margin-bottom: 20px; color: #666;">Improve the structure, clarity, and flow of your text using GPT-4o.</p>
                
                <div class="form-group">
                    <label for="readability-text">Text to enhance:</label>
                    <textarea id="readability-text" placeholder="Enter your text here..."></textarea>
                </div>
                
                <button class="btn" onclick="processReadability()">Enhance Readability</button>
                
                <div id="readability-result" class="result" style="display: none;">
                    <h3>Enhanced Text:</h3>
                    <div class="result-content" id="readability-content"></div>
                </div>
            </div>

            <!-- Ask AI Tab -->
            <div id="ask-ai" class="tab-content">
                <h2>Ask AI</h2>
                <p style="margin-bottom: 20px; color: #666;">Get thoughtful insights and perspectives using O1-mini.</p>
                
                <div class="form-group">
                    <label for="ask-ai-text">Your question or statement:</label>
                    <textarea id="ask-ai-text" placeholder="Ask a question or share a statement..."></textarea>
                </div>
                
                <button class="btn" onclick="processAskAI()">Ask AI</button>
                
                <div id="ask-ai-result" class="result" style="display: none;">
                    <h3>AI Response:</h3>
                    <div class="result-content" id="ask-ai-content"></div>
                </div>
            </div>

            <!-- Correctness Tab -->
            <div id="correctness" class="tab-content">
                <h2>Fact Check</h2>
                <p style="margin-bottom: 20px; color: #666;">Analyze text for factual accuracy using GPT-4o.</p>
                
                <div class="form-group">
                    <label for="correctness-text">Text to fact-check:</label>
                    <textarea id="correctness-text" placeholder="Enter text to check for factual accuracy..."></textarea>
                </div>
                
                <button class="btn" onclick="processCorrectness()">Check Facts</button>
                
                <div id="correctness-result" class="result" style="display: none;">
                    <h3>Factual Analysis:</h3>
                    <div class="result-content" id="correctness-content"></div>
                </div>
            </div>

            <!-- Real-time Audio Tab -->
            <div id="realtime" class="tab-content">
                <h2>üé§ Real-time Audio Transcription</h2>
                <p style="margin-bottom: 20px; color: #666;">Record audio and get real-time transcription with AI processing.</p>
                
                <div class="recording-status" id="recordingStatus" style="display: none;">
                    <span id="recordingStatusText"></span>
                </div>
                
                <div class="audio-controls">
                    <button class="btn" id="startRecording" onclick="startRecording()">üé§ Start Recording</button>
                    <button class="btn stop" id="stopRecording" onclick="stopRecording()" style="display: none;">‚èπÔ∏è Stop Recording</button>
                </div>
                
                <div class="form-group">
                    <label>Transcription:</label>
                    <textarea id="transcription-text" placeholder="Transcription will appear here..." readonly></textarea>
                </div>
                
                <div class="form-group">
                    <label>AI Response:</label>
                    <textarea id="ai-response-text" placeholder="AI response will appear here..." readonly></textarea>
                </div>
                
                <div id="realtime-result" class="result" style="display: none;">
                    <h3>Processing Result:</h3>
                    <div class="result-content" id="realtime-content"></div>
                </div>
            </div>
        </div>
    </div>

    <script>
        let mediaRecorder;
        let audioChunks = [];
        let sessionId = null;
        let isRecording = false;
        let pollingInterval = null;

        // Check API status on load
        window.onload = function() {
            checkApiStatus();
        };

        function checkApiStatus() {
            fetch('/api/status')
                .then(response => response.json())
                .then(data => {
                    document.getElementById('apiStatus').className = 'api-status online';
                    document.getElementById('statusText').textContent = 'API is online and ready';
                    document.getElementById('status').style.background = '#e8f5e8';
                    document.getElementById('status').style.color = '#2d5a2d';
                })
                .catch(error => {
                    document.getElementById('apiStatus').className = 'api-status offline';
                    document.getElementById('statusText').textContent = 'API is offline';
                    document.getElementById('status').style.background = '#fee';
                    document.getElementById('status').style.color = '#c0392b';
                });
        }

        function showTab(tabName) {
            // Hide all tab contents
            const tabContents = document.querySelectorAll('.tab-content');
            tabContents.forEach(content => content.classList.remove('active'));
            
            // Remove active class from all tabs
            const tabs = document.querySelectorAll('.tab');
            tabs.forEach(tab => tab.classList.remove('active'));
            
            // Show selected tab content
            document.getElementById(tabName).classList.add('active');
            
            // Add active class to clicked tab
            event.target.classList.add('active');
        }

        async function startRecording() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                mediaRecorder = new MediaRecorder(stream);
                audioChunks = [];
                
                mediaRecorder.ondataavailable = (event) => {
                    audioChunks.push(event.data);
                };
                
                mediaRecorder.onstop = async () => {
                    const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
                    await uploadAudio(audioBlob);
                };
                
                mediaRecorder.start();
                isRecording = true;
                
                document.getElementById('startRecording').style.display = 'none';
                document.getElementById('stopRecording').style.display = 'inline-block';
                document.getElementById('recordingStatus').style.display = 'block';
                document.getElementById('recordingStatus').className = 'recording-status recording';
                document.getElementById('recordingStatusText').textContent = 'üé§ Recording...';
                
            } catch (error) {
                alert('Error accessing microphone: ' + error.message);
            }
        }

        function stopRecording() {
            if (mediaRecorder && isRecording) {
                mediaRecorder.stop();
                isRecording = false;
                
                document.getElementById('startRecording').style.display = 'inline-block';
                document.getElementById('stopRecording').style.display = 'none';
                document.getElementById('recordingStatus').className = 'recording-status processing';
                document.getElementById('recordingStatusText').textContent = '‚è≥ Processing audio...';
            }
        }

        async function uploadAudio(audioBlob) {
            try {
                const formData = new FormData();
                formData.append('audio', audioBlob, 'recording.wav');
                
                const response = await fetch('/api/v1/audio/upload', {
                    method: 'POST',
                    body: formData
                });
                
                const data = await response.json();
                
                if (data.session_id) {
                    sessionId = data.session_id;
                    startPolling();
                } else {
                    throw new Error(data.error || 'Failed to upload audio');
                }
                
            } catch (error) {
                document.getElementById('recordingStatus').className = 'recording-status error';
                document.getElementById('recordingStatusText').textContent = '‚ùå Error: ' + error.message;
            }
        }

        function startPolling() {
            if (pollingInterval) clearInterval(pollingInterval);
            
            pollingInterval = setInterval(async () => {
                try {
                    const response = await fetch(`/api/v1/audio/status/${sessionId}`);
                    const data = await response.json();
                    
                    if (data.status === 'completed') {
                        clearInterval(pollingInterval);
                        document.getElementById('recordingStatus').className = 'recording-status complete';
                        document.getElementById('recordingStatusText').textContent = '‚úÖ Processing complete!';
                        
                        // Display results
                        if (data.transcription) {
                            document.getElementById('transcription-text').value = data.transcription;
                        }
                        if (data.ai_response) {
                            document.getElementById('ai-response-text').value = data.ai_response;
                        }
                        
                    } else if (data.status === 'processing') {
                        document.getElementById('recordingStatusText').textContent = '‚è≥ Processing audio...';
                        
                    } else if (data.status === 'error') {
                        clearInterval(pollingInterval);
                        document.getElementById('recordingStatus').className = 'recording-status error';
                        document.getElementById('recordingStatusText').textContent = '‚ùå Error: ' + (data.error || 'Processing failed');
                    }
                    
                } catch (error) {
                    console.error('Polling error:', error);
                }
            }, 1000); // Poll every second
        }

        function processReadability() {
            const text = document.getElementById('readability-text').value.trim();
            if (!text) {
                alert('Please enter some text to enhance.');
                return;
            }

            const btn = event.target;
            btn.disabled = true;
            btn.textContent = 'Processing...';

            fetch('/api/v1/readability', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({ text: text })
            })
            .then(response => response.json())
            .then(data => {
                if (data.error) {
                    throw new Error(data.message || 'An error occurred');
                }
                document.getElementById('readability-content').textContent = data.enhanced_text;
                document.getElementById('readability-result').style.display = 'block';
            })
            .catch(error => {
                document.getElementById('readability-content').textContent = 'Error: ' + error.message;
                document.getElementById('readability-result').classList.add('error');
                document.getElementById('readability-result').style.display = 'block';
            })
            .finally(() => {
                btn.disabled = false;
                btn.textContent = 'Enhance Readability';
            });
        }

        function processAskAI() {
            const text = document.getElementById('ask-ai-text').value.trim();
            if (!text) {
                alert('Please enter a question or statement.');
                return;
            }

            const btn = event.target;
            btn.disabled = true;
            btn.textContent = 'Processing...';

            fetch('/api/v1/ask_ai', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({ text: text })
            })
            .then(response => response.json())
            .then(data => {
                if (data.error) {
                    throw new Error(data.message || 'An error occurred');
                }
                document.getElementById('ask-ai-content').textContent = data.answer;
                document.getElementById('ask-ai-result').style.display = 'block';
            })
            .catch(error => {
                document.getElementById('ask-ai-content').textContent = 'Error: ' + error.message;
                document.getElementById('ask-ai-result').classList.add('error');
                document.getElementById('ask-ai-result').style.display = 'block';
            })
            .finally(() => {
                btn.disabled = false;
                btn.textContent = 'Ask AI';
            });
        }

        function processCorrectness() {
            const text = document.getElementById('correctness-text').value.trim();
            if (!text) {
                alert('Please enter text to fact-check.');
                return;
            }

            const btn = event.target;
            btn.disabled = true;
            btn.textContent = 'Processing...';

            fetch('/api/v1/correctness', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({ text: text })
            })
            .then(response => response.json())
            .then(data => {
                if (data.error) {
                    throw new Error(data.message || 'An error occurred');
                }
                document.getElementById('correctness-content').textContent = data.analysis;
                document.getElementById('correctness-result').style.display = 'block';
            })
            .catch(error => {
                document.getElementById('correctness-content').textContent = 'Error: ' + error.message;
                document.getElementById('correctness-result').classList.add('error');
                document.getElementById('correctness-result').style.display = 'block';
            })
            .finally(() => {
                btn.disabled = false;
                btn.textContent = 'Check Facts';
            });
        }
    </script>
</body>
</html>"""
                
                self.wfile.write(html_content.encode('utf-8'))
                return
            
            # API endpoints
            if self.path == "/api/status":
                self.send_response(200)
                self.send_header('Content-type', 'application/json')
                self.send_header('Access-Control-Allow-Origin', '*')
                self.end_headers()
                
                response = {
                    "message": "Brainwave API is running",
                    "status": "healthy",
                    "path": self.path,
                    "environment": "vercel",
                    "endpoints": {
                        "readability": "/api/v1/readability",
                        "ask_ai": "/api/v1/ask_ai", 
                        "correctness": "/api/v1/correctness",
                        "audio_upload": "/api/v1/audio/upload",
                        "audio_status": "/api/v1/audio/status/{session_id}"
                    }
                }
                
                self.wfile.write(json.dumps(response).encode())
                return
            elif self.path.startswith("/api/v1/audio/status/"):
                session_id = self.path.split("/")[-1]
                self.handle_audio_status(session_id)
                return
            else:
                self.send_response(200)
                self.send_header('Content-type', 'application/json')
                self.send_header('Access-Control-Allow-Origin', '*')
                self.end_headers()
                
                response = {
                    "message": "Brainwave API is running",
                    "status": "healthy",
                    "path": self.path,
                    "environment": "vercel",
                    "endpoints": {
                        "readability": "/api/v1/readability",
                        "ask_ai": "/api/v1/ask_ai", 
                        "correctness": "/api/v1/correctness"
                    }
                }
                
                self.wfile.write(json.dumps(response).encode())
            
        except Exception as e:
            self.send_response(500)
            self.send_header('Content-type', 'application/json')
            self.end_headers()
            error_response = {
                "error": "Internal server error",
                "message": str(e)
            }
            self.wfile.write(json.dumps(error_response).encode())
    
    def do_POST(self):
        try:
            # Handle audio upload
            if self.path == "/api/v1/audio/upload":
                self.handle_audio_upload()
                return
            
            # Get content length
            content_length = int(self.headers.get('Content-Length', 0))
            post_data = self.rfile.read(content_length)
            
            # Parse JSON request
            request_data = json.loads(post_data.decode('utf-8'))
            
            # Initialize OpenAI client
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise Exception("OPENAI_API_KEY not found in environment variables")
            
            client = OpenAI(api_key=api_key)
            
            # Route to appropriate endpoint
            if self.path == "/api/v1/readability":
                response = self.handle_readability(client, request_data)
            elif self.path == "/api/v1/ask_ai":
                response = self.handle_ask_ai(client, request_data)
            elif self.path == "/api/v1/correctness":
                response = self.handle_correctness(client, request_data)
            else:
                response = {
                    "message": "POST request received",
                    "path": self.path,
                    "method": "POST"
                }
            
            self.send_response(200)
            self.send_header('Content-type', 'application/json')
            self.send_header('Access-Control-Allow-Origin', '*')
            self.end_headers()
            self.wfile.write(json.dumps(response).encode())
            
        except Exception as e:
            self.send_response(500)
            self.send_header('Content-type', 'application/json')
            self.end_headers()
            error_response = {
                "error": "Internal server error",
                "message": str(e)
            }
            self.wfile.write(json.dumps(error_response).encode())
    
    def handle_audio_upload(self):
        try:
            # Parse multipart form data
            content_length = int(self.headers.get('Content-Length', 0))
            post_data = self.rfile.read(content_length)
            
            # Generate session ID
            session_id = str(uuid.uuid4())
            
            # Store session info (in production, use Redis or database)
            audio_sessions[session_id] = {
                "status": "processing",
                "created_at": time.time(),
                "transcription": None,
                "ai_response": None
            }
            
            # Process audio with OpenAI Whisper
            import threading
            def process_audio():
                try:
                    # Initialize OpenAI client
                    api_key = os.getenv("OPENAI_API_KEY")
                    if not api_key:
                        audio_sessions[session_id]["status"] = "error"
                        audio_sessions[session_id]["error"] = "OPENAI_API_KEY not found"
                        return
                    
                    client = OpenAI(api_key=api_key)
                    
                    # For now, we'll simulate the audio processing
                    # In a real implementation, you would:
                    # 1. Parse the multipart form data properly
                    # 2. Save the audio file temporarily
                    # 3. Use OpenAI's Whisper API to transcribe
                    # 4. Use GPT to generate AI response
                    
                    import time
                    time.sleep(2)  # Simulate processing time
                    
                    # Simulate transcription (replace with real Whisper API call)
                    transcription = "This is a simulated transcription of your audio recording. In a real implementation, this would be the actual transcription from OpenAI's Whisper API."
                    
                    # Generate AI response based on transcription
                    response = client.chat.completions.create(
                        model="gpt-4o",
                        messages=[
                            {"role": "system", "content": "You are a helpful AI assistant. Analyze the transcribed audio and provide thoughtful insights, answer questions, or offer helpful suggestions based on the content."},
                            {"role": "user", "content": f"Based on this audio transcription, please provide a helpful response: {transcription}"}
                        ]
                    )
                    
                    ai_response = response.choices[0].message.content
                    
                    # Update session with results
                    audio_sessions[session_id]["transcription"] = transcription
                    audio_sessions[session_id]["ai_response"] = ai_response
                    audio_sessions[session_id]["status"] = "completed"
                    
                except Exception as e:
                    audio_sessions[session_id]["status"] = "error"
                    audio_sessions[session_id]["error"] = str(e)
            
            threading.Thread(target=process_audio).start()
            
            self.send_response(200)
            self.send_header('Content-type', 'application/json')
            self.send_header('Access-Control-Allow-Origin', '*')
            self.end_headers()
            
            response = {
                "session_id": session_id,
                "status": "processing"
            }
            
            self.wfile.write(json.dumps(response).encode())
            
        except Exception as e:
            self.send_response(500)
            self.send_header('Content-type', 'application/json')
            self.end_headers()
            error_response = {
                "error": "Failed to upload audio",
                "message": str(e)
            }
            self.wfile.write(json.dumps(error_response).encode())
    
    def handle_audio_status(self, session_id):
        try:
            if session_id not in audio_sessions:
                self.send_response(404)
                self.send_header('Content-type', 'application/json')
                self.end_headers()
                error_response = {
                    "error": "Session not found",
                    "message": "Audio session not found"
                }
                self.wfile.write(json.dumps(error_response).encode())
                return
            
            session = audio_sessions[session_id]
            
            self.send_response(200)
            self.send_header('Content-type', 'application/json')
            self.send_header('Access-Control-Allow-Origin', '*')
            self.end_headers()
            
            response = {
                "session_id": session_id,
                "status": session["status"],
                "transcription": session.get("transcription"),
                "ai_response": session.get("ai_response")
            }
            
            self.wfile.write(json.dumps(response).encode())
            
        except Exception as e:
            self.send_response(500)
            self.send_header('Content-type', 'application/json')
            self.end_headers()
            error_response = {
                "error": "Failed to get status",
                "message": str(e)
            }
            self.wfile.write(json.dumps(error_response).encode())
    
    def handle_readability(self, client, request_data):
        text = request_data.get('text', '')
        if not text:
            raise Exception("Text field is required")
        
        prompt = """Improve the readability of the user input text. Enhance the structure, clarity, and flow without altering the original meaning. Correct any grammar and punctuation errors, and ensure that the text is well-organized and easy to understand. It's important to achieve a balance between easy-to-digest, thoughtful, insightful, and not overly formal. We're not writing a column article appearing in The New York Times. Instead, the audience would mostly be friendly colleagues or online audiences. Therefore, you need to, on one hand, make sure the content is easy to digest and accept. On the other hand, it needs to present insights and best to have some surprising and deep points. Do not add any additional information or change the intent of the original content. Don't respond to any questions or requests in the conversation. Just treat them literally and correct any mistakes. Don't translate any part of the text, even if it's a mixture of multiple languages. Only output the revised text, without any any other explanation. Reply in the same language as the user input (text to be processed).

Below is the text to be processed:"""
        
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "user", "content": f"{prompt}\n\n{text}"}
            ]
        )
        
        return {
            "enhanced_text": response.choices[0].message.content
        }
    
    def handle_ask_ai(self, client, request_data):
        text = request_data.get('text', '')
        if not text:
            raise Exception("Text field is required")
        
        prompt = """You're an AI assistant skilled in persuasion and offering thoughtful perspectives. When you read through user-provided text, ensure you understand its content thoroughly. Reply in the same language as the user input (text from the user). If it's a question, respond insightfully and deeply. If it's a statement, consider two things: 
        
        first, how can you extend this topic to enhance its depth and convincing power? Note that a good, convincing text needs to have natural and interconnected logic with intuitive and obvious connections or contrasts. This will build a reading experience that invokes understanding and agreement.
        
        Second, can you offer a thought-provoking challenge to the user's perspective? Your response doesn't need to be exhaustive or overly detailed. The main goal is to inspire thought and easily convince the audience. Embrace surprising and creative angles.

Below is the text from the user:"""
        
        response = client.chat.completions.create(
            model="o1-mini",
            messages=[
                {"role": "user", "content": f"{prompt}\n\n{text}"}
            ]
        )
        
        return {
            "answer": response.choices[0].message.content
        }
    
    def handle_correctness(self, client, request_data):
        text = request_data.get('text', '')
        if not text:
            raise Exception("Text field is required")
        
        prompt = """Analyze the following text for factual accuracy. Reply in the same language as the user input (text to analyze). Focus on:
1. Identifying any factual errors or inaccurate statements
2. Checking the accuracy of any claims or assertions

Provide a clear, concise response that:
- Points out any inaccuracies found
- Suggests corrections where needed
- Confirms accurate statements
- Flags any claims that need verification

Keep the tone professional but friendly. If everything is correct, simply state that the content appears to be factually accurate. 

Below is the text to analyze:"""
        
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "user", "content": f"{prompt}\n\n{text}"}
            ]
        )
        
        return {
            "analysis": response.choices[0].message.content
        } 